{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "79d1763d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import uuid\n",
    "\n",
    "import sagemaker\n",
    "from sagemaker.pytorch import PyTorch\n",
    "from sagemaker import get_execution_role"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1b4b7b51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/JupyterSystemEnv/bin/python: Error while finding module specification for 'torch.distributed.launch' (ModuleNotFoundError: No module named 'torch')\n"
     ]
    }
   ],
   "source": [
    "!python -m torch.distributed.launch --nproc_per_node=8 --nnodes=1 --node_rank=0 \\\n",
    "        /home/ec2-user/src/pytorch_resnet_cifar10_mirror/trainer.py --use_adascale --num_epochs 200 \\\n",
    "        --batch_size 256 \\\n",
    "        --weight_decay 1e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87e23b73",
   "metadata": {},
   "outputs": [],
   "source": [
    "!  mkdir -p data && cd data && wget -c --quiet https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz && tar -xvzf cifar-10-python.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90bad4ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "! mkdir -p saved_models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2072049e",
   "metadata": {},
   "source": [
    "## Run locally multiGPU training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a60f68d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/JupyterSystemEnv/bin/python: Error while finding module specification for 'torch.distributed.launch' (ModuleNotFoundError: No module named 'torch')\n"
     ]
    }
   ],
   "source": [
    "!python -m torch.distributed.launch --nproc_per_node=8 --nnodes=1 --node_rank=0 \\\n",
    "        /home/ec2-user/SageMaker/pytorch_resnet_cifar10_mirror/cifar.py  --num_epochs 200 \\\n",
    "        --batch_size 256 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a3f340e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Sagemaker ddp training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13c6eecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e8f2995",
   "metadata": {},
   "outputs": [],
   "source": [
    "! ls /home/ec2-user/SageMaker/pytorch_resnet_cifar10_mirror/sm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "53e4c182",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Data directory:  ['cifar-10-batches-py', 'cifar-10-python.tar.gz']\n",
      "*****************************************\n",
      "Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \n",
      "*****************************************\n",
      "NCCL version 2.7.8+cuda10.1\n",
      " ### data_dir  /home/ec2-user/SageMaker/data/\n",
      " ### data_dir  /home/ec2-user/SageMaker/data/\n",
      " ### data_dir  /home/ec2-user/SageMaker/data/\n",
      " ### data_dir  /home/ec2-user/SageMaker/data/\n",
      " ### data_dir  /home/ec2-user/SageMaker/data/\n",
      " ### data_dir  /home/ec2-user/SageMaker/data/\n",
      " ### data_dir  /home/ec2-user/SageMaker/data/\n",
      " ### data_dir  /home/ec2-user/SageMaker/data/\n",
      "Local Rank: 2, Epoch: 0, Training ...\n",
      "Local Rank: 7, Epoch: 0, Training ...\n",
      "Local Rank: 3, Epoch: 0, Training ...\n",
      "Local Rank: 1, Epoch: 0, Training ...\n",
      "Local Rank: 0, Epoch: 0, Training ...\n",
      "Local Rank: 4, Epoch: 0, Training ...\n",
      "Local Rank: 6, Epoch: 0, Training ...\n",
      "Local Rank: 5, Epoch: 0, Training ...\n",
      "---------------------------------------------------------------------------\n",
      "Epoch: 0, Accuracy: 0.0\n",
      "---------------------------------------------------------------------------\n",
      "Local Rank: 5, Epoch: 1, Training ...\n",
      "Local Rank: 7, Epoch: 1, Training ...\n",
      "Local Rank: 2, Epoch: 1, Training ...\n",
      "Local Rank: 3, Epoch: 1, Training ...\n",
      "Local Rank: 0, Epoch: 1, Training ...\n",
      "Local Rank: 1, Epoch: 1, Training ...\n",
      "Local Rank: 4, Epoch: 1, Training ...\n",
      "Local Rank: 6, Epoch: 1, Training ...\n",
      "Local Rank: 5, Epoch: 2, Training ...\n",
      "Local Rank: 1, Epoch: 2, Training ...\n",
      "Local Rank: 3, Epoch: 2, Training ...\n",
      "Local Rank: 4, Epoch: 2, Training ...\n",
      "Local Rank: 0, Epoch: 2, Training ...\n",
      "Local Rank: 6, Epoch: 2, Training ...\n",
      "Local Rank: 2, Epoch: 2, Training ...\n",
      "Local Rank: 7, Epoch: 2, Training ...\n",
      "Local Rank: 2, Epoch: 3, Training ...\n",
      "Local Rank: 1, Epoch: 3, Training ...\n",
      "Local Rank: 5, Epoch: 3, Training ...\n",
      "Local Rank: 7, Epoch: 3, Training ...\n",
      "Local Rank: 4, Epoch: 3, Training ...\n",
      "Local Rank: 3, Epoch: 3, Training ...\n",
      "Local Rank: 6, Epoch: 3, Training ...\n",
      "Local Rank: 0, Epoch: 3, Training ...\n",
      "Local Rank: 1, Epoch: 4, Training ...\n",
      "Local Rank: 4, Epoch: 4, Training ...\n",
      "Local Rank: 7, Epoch: 4, Training ...\n",
      "Local Rank: 3, Epoch: 4, Training ...\n",
      "Local Rank: 0, Epoch: 4, Training ...\n",
      "Local Rank: 5, Epoch: 4, Training ...\n",
      "Local Rank: 2, Epoch: 4, Training ...\n",
      "Local Rank: 6, Epoch: 4, Training ...\n",
      "Local Rank: 5, Epoch: 5, Training ...\n",
      "Local Rank: 4, Epoch: 5, Training ...\n",
      "Local Rank: 0, Epoch: 5, Training ...\n",
      "Local Rank: 1, Epoch: 5, Training ...\n",
      "Local Rank: 6, Epoch: 5, Training ...\n",
      "Local Rank: 3, Epoch: 5, Training ...\n",
      "Local Rank: 7, Epoch: 5, Training ...\n",
      "Local Rank: 2, Epoch: 5, Training ...\n",
      "Local Rank: 7, Epoch: 6, Training ...\n",
      "Local Rank: 5, Epoch: 6, Training ...\n",
      "Local Rank: 3, Epoch: 6, Training ...\n",
      "Local Rank: 0, Epoch: 6, Training ...\n",
      "Local Rank: 1, Epoch: 6, Training ...\n",
      "Local Rank: 4, Epoch: 6, Training ...\n",
      "Local Rank: 6, Epoch: 6, Training ...\n",
      "Local Rank: 2, Epoch: 6, Training ...\n",
      "Local Rank: 0, Epoch: 7, Training ...\n",
      "Local Rank: 5, Epoch: 7, Training ...\n",
      "Local Rank: 1, Epoch: 7, Training ...\n",
      "Local Rank: 2, Epoch: 7, Training ...\n",
      "Local Rank: 4, Epoch: 7, Training ...\n",
      "Local Rank: 3, Epoch: 7, Training ...\n",
      "Local Rank: 7, Epoch: 7, Training ...\n",
      "Local Rank: 6, Epoch: 7, Training ...\n",
      "Local Rank: 5, Epoch: 8, Training ...\n",
      "Local Rank: 2, Epoch: 8, Training ...\n",
      "Local Rank: 3, Epoch: 8, Training ...\n",
      "Local Rank: 4, Epoch: 8, Training ...\n",
      "Local Rank: 1, Epoch: 8, Training ...\n",
      "Local Rank: 0, Epoch: 8, Training ...\n",
      "Local Rank: 6, Epoch: 8, Training ...\n",
      "Local Rank: 7, Epoch: 8, Training ...\n",
      "Local Rank: 5, Epoch: 9, Training ...\n",
      "Local Rank: 3, Epoch: 9, Training ...\n",
      "Local Rank: 7, Epoch: 9, Training ...\n",
      "Local Rank: 4, Epoch: 9, Training ...\n",
      "Local Rank: 1, Epoch: 9, Training ...\n",
      "Local Rank: 0, Epoch: 9, Training ...\n",
      "Local Rank: 2, Epoch: 9, Training ...\n",
      "Local Rank: 6, Epoch: 9, Training ...\n",
      "Local Rank: 1, Epoch: 10, Training ...\n",
      "Local Rank: 3, Epoch: 10, Training ...\n",
      "Local Rank: 7, Epoch: 10, Training ...\n",
      "Local Rank: 5, Epoch: 10, Training ...\n",
      "Local Rank: 2, Epoch: 10, Training ...\n",
      "Local Rank: 0, Epoch: 10, Training ...\n",
      "Local Rank: 4, Epoch: 10, Training ...\n",
      "Local Rank: 6, Epoch: 10, Training ...\n",
      "---------------------------------------------------------------------------\n",
      "Epoch: 10, Accuracy: 0.5418\n",
      "---------------------------------------------------------------------------\n",
      "Local Rank: 7, Epoch: 11, Training ...\n",
      "Local Rank: 5, Epoch: 11, Training ...\n",
      "Local Rank: 3, Epoch: 11, Training ...\n",
      "Local Rank: 2, Epoch: 11, Training ...\n",
      "Local Rank: 0, Epoch: 11, Training ...\n",
      "Local Rank: 6, Epoch: 11, Training ...\n",
      "Local Rank: 4, Epoch: 11, Training ...\n",
      "Local Rank: 1, Epoch: 11, Training ...\n",
      "Local Rank: 2, Epoch: 12, Training ...\n",
      "Local Rank: 5, Epoch: 12, Training ...\n",
      "Local Rank: 0, Epoch: 12, Training ...\n",
      "Local Rank: 3, Epoch: 12, Training ...\n",
      "Local Rank: 1, Epoch: 12, Training ...\n",
      "Local Rank: 6, Epoch: 12, Training ...\n",
      "Local Rank: 7, Epoch: 12, Training ...\n",
      "Local Rank: 4, Epoch: 12, Training ...\n",
      "Local Rank: 1, Epoch: 13, Training ...\n",
      "Local Rank: 0, Epoch: 13, Training ...\n",
      "Local Rank: 4, Epoch: 13, Training ...\n",
      "Local Rank: 7, Epoch: 13, Training ...\n",
      "Local Rank: 3, Epoch: 13, Training ...\n",
      "Local Rank: 5, Epoch: 13, Training ...\n",
      "Local Rank: 6, Epoch: 13, Training ...\n",
      "Local Rank: 2, Epoch: 13, Training ...\n",
      "Local Rank: 6, Epoch: 14, Training ...\n",
      "Local Rank: 3, Epoch: 14, Training ...\n",
      "Local Rank: 5, Epoch: 14, Training ...\n",
      "Local Rank: 2, Epoch: 14, Training ...\n",
      "Local Rank: 7, Epoch: 14, Training ...\n",
      "Local Rank: 4, Epoch: 14, Training ...\n",
      "Local Rank: 1, Epoch: 14, Training ...\n",
      "Local Rank: 0, Epoch: 14, Training ...\n",
      "Local Rank: 5, Epoch: 15, Training ...\n",
      "Local Rank: 4, Epoch: 15, Training ...\n",
      "Local Rank: 6, Epoch: 15, Training ...\n",
      "Local Rank: 2, Epoch: 15, Training ...\n",
      "Local Rank: 3, Epoch: 15, Training ...\n",
      "Local Rank: 1, Epoch: 15, Training ...\n",
      "Local Rank: 0, Epoch: 15, Training ...\n",
      "Local Rank: 7, Epoch: 15, Training ...\n",
      "Local Rank: 5, Epoch: 16, Training ...\n",
      "Local Rank: 6, Epoch: 16, Training ...\n",
      "Local Rank: 0, Epoch: 16, Training ...\n",
      "Local Rank: 1, Epoch: 16, Training ...\n",
      "Local Rank: 7, Epoch: 16, Training ...\n",
      "Local Rank: 2, Epoch: 16, Training ...\n",
      "Local Rank: 3, Epoch: 16, Training ...\n",
      "Local Rank: 4, Epoch: 16, Training ...\n",
      "Local Rank: 2, Epoch: 17, Training ...\n",
      "Local Rank: 3, Epoch: 17, Training ...\n",
      "Local Rank: 5, Epoch: 17, Training ...\n",
      "Local Rank: 4, Epoch: 17, Training ...\n",
      "Local Rank: 7, Epoch: 17, Training ...\n",
      "Local Rank: 6, Epoch: 17, Training ...\n",
      "Local Rank: 0, Epoch: 17, Training ...\n",
      "Local Rank: 1, Epoch: 17, Training ...\n",
      "Local Rank: 6, Epoch: 18, Training ...\n",
      "Local Rank: 3, Epoch: 18, Training ...\n",
      "Local Rank: 4, Epoch: 18, Training ...\n",
      "Local Rank: 1, Epoch: 18, Training ...\n",
      "Local Rank: 0, Epoch: 18, Training ...\n",
      "Local Rank: 7, Epoch: 18, Training ...\n",
      "Local Rank: 5, Epoch: 18, Training ...\n",
      "Local Rank: 2, Epoch: 18, Training ...\n",
      "Local Rank: 5, Epoch: 19, Training ...\n",
      "Local Rank: 6, Epoch: 19, Training ...\n",
      "Local Rank: 7, Epoch: 19, Training ...\n",
      "Local Rank: 1, Epoch: 19, Training ...\n",
      "Local Rank: 2, Epoch: 19, Training ...\n",
      "Local Rank: 4, Epoch: 19, Training ...\n",
      "Local Rank: 0, Epoch: 19, Training ...\n",
      "Local Rank: 3, Epoch: 19, Training ...\n",
      "^C\n",
      "Traceback (most recent call last):\n",
      "  File \"ddp-launcher.py\", line 119, in <module>\n",
      "    '--data_dir', args.data_dir \n",
      "  File \"/home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/subprocess.py\", line 289, in call\n",
      "    return p.wait(timeout=timeout)\n",
      "  File \"/home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/subprocess.py\", line 1477, in wait\n",
      "    (pid, sts) = self._try_wait(0)\n",
      "  File \"/home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/subprocess.py\", line 1424, in _try_wait\n",
      "    (pid, sts) = os.waitpid(self.pid, wait_flags)\n",
      "KeyboardInterrupt\n",
      "Traceback (most recent call last):\n",
      "  File \"cifar.py\", line 161, in <module>\n",
      "    main()\n",
      "  File \"cifar.py\", line 155, in main\n",
      "    loss.backward()\n",
      "  File \"/home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages/torch/tensor.py\", line 221, in backward\n",
      "Traceback (most recent call last):\n",
      "  File \"cifar.py\", line 161, in <module>\n",
      "    main()\n",
      "  File \"cifar.py\", line 155, in main\n",
      "    loss.backward()\n",
      "  File \"/home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages/torch/tensor.py\", line 221, in backward\n",
      "Traceback (most recent call last):\n",
      "  File \"cifar.py\", line 161, in <module>\n",
      "    main()\n",
      "  File \"cifar.py\", line 155, in main\n",
      "    loss.backward()\n",
      "  File \"/home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages/torch/tensor.py\", line 221, in backward\n"
     ]
    }
   ],
   "source": [
    "! cd /home/ec2-user/SageMaker/pytorch_resnet_cifar10_mirror/sm &&  python ddp-launcher.py --gpus 8 \\\n",
    "--data_dir /home/ec2-user/SageMaker/data/ \\\n",
    "--model_dir /home/ec2-user/SageMaker/ \\\n",
    "--num_epochs 100\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f3b9fe4",
   "metadata": {},
   "source": [
    "## Multinode multi GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5a96d486",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    'batch_size': 256,\n",
    "    'num_epochs' : 50}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b53476f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket = 'mansmane-us-west-2'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "987bfab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training time of this job \n",
    "token = str(uuid.uuid4())[:10]  # we create a unique token to avoid checkpoint collisions in S3\n",
    "\n",
    "job = PyTorch(\n",
    "    entry_point='ddp-launcher.py',\n",
    "    source_dir='/home/ec2-user/SageMaker/pytorch_resnet_cifar10_mirror/sm',\n",
    "    role=get_execution_role(),\n",
    "    framework_version='1.8.1',\n",
    "    instance_count=1,\n",
    "    instance_type='ml.p3.16xlarge',\n",
    "    base_job_name='resnet-multi-GPU-g5',\n",
    "    py_version='py36',\n",
    "    hyperparameters=config,\n",
    "    checkpoint_s3_uri='s3://{}/{}/checkpoints'.format(bucket, token),  # S3 destination of /opt/ml/checkpoints files\n",
    "    output_path='s3://{}/{}'.format(bucket, token),\n",
    "    code_location='s3://{}/{}/code'.format(bucket, token), # source_dir code will be staged in S3 there\n",
    "    environment={\"SMDEBUG_LOG_LEVEL\":\"off\"},  # reduce verbosity of Debugger\n",
    "    debugger_hook_config=False,  # deactivate debugger to avoid warnings in model artifact\n",
    "    disable_profiler=True,  # keep running resources to a minimum to avoid permission errors\n",
    "    metric_definitions=[\n",
    "        {\"Name\": \"Train_loss\", \"Regex\": \"Training_loss: ([0-9.]+).*$\"},\n",
    "        {\"Name\": \"Learning_rate\", \"Regex\": \"learning rate: ([0-9.]+).*$\"},        \n",
    "        {\"Name\": \"Val_loss\", \"Regex\": \"Val_loss: ([0-9.]+).*$\"},        \n",
    "        {\"Name\": \"Throughput\", \"Regex\": \"Throughput: ([0-9.]+).*$\"},\n",
    "        {\"Name\": \"Val_pixel_acc\", \"Regex\": \"Val_pixel_acc: ([0-9.]+).*$\"}\n",
    "    ],\n",
    "    tags=[{'Key': 'Project', 'Value': 'A2D2_segmentation'}])  # tag the job for experiment tracking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "000cfb1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = 's3://mansmane-us-west-2/cifar10/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9e6208ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "job.fit({'dataset': train_path}, wait=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82a11df8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ce52a304",
   "metadata": {},
   "outputs": [],
   "source": [
    "token = str(uuid.uuid4())[:10]  # we create a unique token to avoid checkpoint collisions in S3\n",
    "instance_count = 2\n",
    "\n",
    "job = PyTorch(\n",
    "    entry_point='ddp-launcher.py',\n",
    "    source_dir='/home/ec2-user/SageMaker/pytorch_resnet_cifar10_mirror/sm',\n",
    "    role=get_execution_role(),\n",
    "    framework_version='1.8.1',\n",
    "    instance_count=instance_count,\n",
    "    instance_type='ml.p3.16xlarge',\n",
    "    base_job_name='resnet-multi-GPU-g5-instance-' + str(instance_count),\n",
    "    py_version='py36',\n",
    "    hyperparameters=config,\n",
    "    checkpoint_s3_uri='s3://{}/{}/checkpoints'.format(bucket, token),  # S3 destination of /opt/ml/checkpoints files\n",
    "    output_path='s3://{}/{}'.format(bucket, token),\n",
    "    code_location='s3://{}/{}/code'.format(bucket, token), # source_dir code will be staged in S3 there\n",
    "    environment={\"SMDEBUG_LOG_LEVEL\":\"off\"},  # reduce verbosity of Debugger\n",
    "    debugger_hook_config=False,  # deactivate debugger to avoid warnings in model artifact\n",
    "    disable_profiler=True,  # keep running resources to a minimum to avoid permission errors\n",
    "    metric_definitions=[\n",
    "        {\"Name\": \"Train_loss\", \"Regex\": \"Training_loss: ([0-9.]+).*$\"},\n",
    "        {\"Name\": \"Learning_rate\", \"Regex\": \"learning rate: ([0-9.]+).*$\"},        \n",
    "        {\"Name\": \"Val_loss\", \"Regex\": \"Val_loss: ([0-9.]+).*$\"},        \n",
    "        {\"Name\": \"Throughput\", \"Regex\": \"Throughput: ([0-9.]+).*$\"},\n",
    "        {\"Name\": \"Val_pixel_acc\", \"Regex\": \"Val_pixel_acc: ([0-9.]+).*$\"}\n",
    "    ],\n",
    "    tags=[{'Key': 'Project', 'Value': 'A2D2_segmentation'}])  # tag the job for experiment tracking\n",
    "\n",
    "train_path = 's3://mansmane-us-west-2/cifar10/'\n",
    "job.fit({'dataset': train_path}, wait=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebfc4833",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting fairscale\n",
      "  Downloading fairscale-0.4.5.tar.gz (240 kB)\n",
      "     |████████████████████████████████| 240 kB 6.9 MB/s            \n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Installing backend dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting torch>=1.8.0\n",
      "  Downloading torch-1.10.2-cp36-cp36m-manylinux1_x86_64.whl (881.9 MB)\n",
      "     |████████████████████████████▎   | 777.8 MB 95.2 MB/s eta 0:00:02"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     |████████████████████████████████| 881.9 MB 7.8 kB/s             \n",
      "\u001b[?25hRequirement already satisfied: dataclasses in /home/ec2-user/anaconda3/envs/amazonei_pytorch_latest_p36/lib/python3.6/site-packages (from torch>=1.8.0->fairscale) (0.8)\n",
      "Requirement already satisfied: typing-extensions in /home/ec2-user/anaconda3/envs/amazonei_pytorch_latest_p36/lib/python3.6/site-packages (from torch>=1.8.0->fairscale) (3.10.0.0)\n",
      "Building wheels for collected packages: fairscale\n",
      "  Building wheel for fairscale (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for fairscale: filename=fairscale-0.4.5-py3-none-any.whl size=299652 sha256=fab78c7163c9400e55e41affb8d2d419028ed6d79cc046c64972dcef3d15be05\n",
      "  Stored in directory: /home/ec2-user/.cache/pip/wheels/77/41/f5/d42db16aa9cfafa47a9c19c7fd5d9b9e8d2dc523fa9ff94344\n",
      "Successfully built fairscale\n",
      "Installing collected packages: torch, fairscale\n",
      "  Attempting uninstall: torch\n",
      "    Found existing installation: torch 1.5.1\n",
      "    Uninstalling torch-1.5.1:\n",
      "      Successfully uninstalled torch-1.5.1\n"
     ]
    }
   ],
   "source": [
    "! pip install fairscale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad5c5e4f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_latest_p36",
   "language": "python",
   "name": "conda_pytorch_latest_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
